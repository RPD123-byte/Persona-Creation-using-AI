{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum 16 GB VRAM required to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  \"\"\"\n",
    "  Prints the number of trainable parameters in the model.\n",
    "  \"\"\"\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "  print(\n",
    "      f\"trainable params: {trainable_params} || all params: {all_param} || trainables%: {100 * trainable_params / all_param}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "Question: What strategies can help in retaining employees?\n",
    "Source1: reputation is the best thing that you can have if you have given enough to people you will already\\nhave earned their trust  that you don't need to sell they will just say I want to work with you and you\\njust accept it and you say sure the give give first until they ask is the long-term strategy that\\ncompounds unto itself because your business will compound as a result of the Goodwill that\\ncompounds in the marketplace whereas if you have a sales driven Org the only way that that\\nbusiness compounds is if the sales guys compounding their recruiting on your behalf rather than\\nbecause if they have a great product they have a great sales system\\ngenerational wealth is transferred through education not assets ninety percent of the time by Third\\nGeneration The Wealth has been squandered because they never learned how to make the money\\nall they had was they were given the money and it's a lot  if you give a man a fish he eats for a day\\neven if you have lots of fish you still need to teach them how to fish because that generation needs\\nto teach the Next Generation how to fish because the fish do eventually run out and  on that long\\ntime Horizon it's education not the asset itself that transfers it over the Long Haul\\nhere's five things you can do to get anyone to understand what you're trying to tell the first is you\\ntake out adverbs  when people say really very extremely it  adds words and makes it more confusing\\nthe second thing is speaking the passive tone a business is valued by instead of saying that you say\\nwe value a business this way  it goes from passive to active voice the third way is by using shorter\\nwords and shorter sentences instead of having colons and commas just put the period there and\\nstart the next sentence the fourth thing is using shorter and smaller words people have to\\nunderstand what you're  saying and the last thing is you want to use vocabulary that everyone\\nunderstands  rather than saying are multiple different sources i might say lots of stuff\\nbe willing to negotiate everything except for your values I used to think of negotiating as a zero-sum\\ngame and it's not at all you can find a way where both parties get better than they were before which\\nis the entire point that is how a negotiating works if you're doing it well I didn't understand that I\n",
    "Source2: you through the three values that we have at acquisition.com and why we believe them and  from an\\noverarching perspective the way to come to values  is to look at what are the non-negotiables okay\\nand  what i mean by that is and every company is different because the core thing about values is\\nthat they have to be things that are true and innate to you right  for example at southwest  have fun\\nis one of their core values and if someone does not want to have fun or does not believe in the\\nprocesses that they do to have a good time then that is a non-negotiable for them that person\\ncannot work at that company it is a fireable offense and  these are not aspirational these are not\\nthings that you would  to have these are things that are core to who you are as a person and as a\\nteam all right and  when you decide on these non-negotiables they are by their very nature\\nnon-negotiable which means that you hire based on these you fire based on these and these are the\\ncore spirit of the team and some of the mistakes that i see when people make values is that one\\nthey don't draw the line in the sand and  one of the core things about a value is that it has to you\\nhave to be able to say this is not this right you have to be able to say okay between justice and\\nmercy we lean towards justice or we lean towards mercy which means you have to repel people with\\nyour values all right and  if your values do not repel people then they are not values they are\\nplatitudes okay and  it's very important the next thing is that when you're making your values the\\nvalues themselves need to be ideally said in words that you would normally say  if you have little\\nsayings inside of your community then then a lot of times those can become some of the values you\\nhave  for example jim launch one of our first portfolio companies  speed is king do the boring work\\nthese were different ways i mean we could just say work ethic but that's not the way we would have\\nsaid it we could have say be fast or fast turnaround times but speed is king was the way we would\\nhave said it okay and  some of the things that i've learned with this is also you cannot have too\\nmany values as in i'll say it differently you can't have too many values you have to be very selective\\nat the the true core values of the company all right and  if you're  between two a lot of times you just\\nhave to chunk up  and kind of get a broader value but they become the core and i think that what we\\nhave found is that three is the sweet spot and that number has continued to distill down over time\\nbut i think three is the amount that your human brain can comprehend as lenses to make a decision\n",
    "Task: Based on the source1 text and source2 text, answer the question.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Tokenize the input\n",
    "encoding = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding[\"input_ids\"],\n",
    "        attention_mask=encoding[\"attention_mask\"],\n",
    "        max_length=1300,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.7\n",
    "    )\n",
    "\n",
    "# Decode and print the generated response\n",
    "generated_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files = \"prompt_response_pairs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    prompt_text = f\"\"\"\n",
    "    Question: {data_point[\"Prompt\"]}\n",
    "    Source1: {data_point[\"Source1\"]}\n",
    "    Source2: {data_point[\"Source2\"]}\n",
    "    Task: Based on the source1 text and source2 text, answer the question.\n",
    "    Answer: {data_point[\"Response\"]}\n",
    "    \"\"\".strip()\n",
    "    return prompt_text\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  prompt_text = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = tokenizer(prompt_text, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "      per_device_train_batch_size=2,\n",
    "      gradient_accumulation_steps=4,\n",
    "      num_train_epochs=2,\n",
    "      learning_rate=2e-3,\n",
    "      fp16=True,\n",
    "      save_total_limit=3,\n",
    "      logging_steps=1,\n",
    "      output_dir=\"experiments\",\n",
    "      optim=\"paged_adamw_8bit\",\n",
    "      lr_scheduler_type=\"cosine\",\n",
    "      warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_MODEL = \"username/tunedpersona-falcon-7b\"\n",
    "\n",
    "model.push_to_hub(\n",
    "    PEFT_MODEL, use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "Question: What strategies can help in retaining employees?\n",
    "Source1: reputation is the best thing that you can have if you have given enough to people you will already\\nhave earned their trust  that you don't need to sell they will just say I want to work with you and you\\njust accept it and you say sure the give give first until they ask is the long-term strategy that\\ncompounds unto itself because your business will compound as a result of the Goodwill that\\ncompounds in the marketplace whereas if you have a sales driven Org the only way that that\\nbusiness compounds is if the sales guys compounding their recruiting on your behalf rather than\\nbecause if they have a great product they have a great sales system\\ngenerational wealth is transferred through education not assets ninety percent of the time by Third\\nGeneration The Wealth has been squandered because they never learned how to make the money\\nall they had was they were given the money and it's a lot  if you give a man a fish he eats for a day\\neven if you have lots of fish you still need to teach them how to fish because that generation needs\\nto teach the Next Generation how to fish because the fish do eventually run out and  on that long\\ntime Horizon it's education not the asset itself that transfers it over the Long Haul\\nhere's five things you can do to get anyone to understand what you're trying to tell the first is you\\ntake out adverbs  when people say really very extremely it  adds words and makes it more confusing\\nthe second thing is speaking the passive tone a business is valued by instead of saying that you say\\nwe value a business this way  it goes from passive to active voice the third way is by using shorter\\nwords and shorter sentences instead of having colons and commas just put the period there and\\nstart the next sentence the fourth thing is using shorter and smaller words people have to\\nunderstand what you're  saying and the last thing is you want to use vocabulary that everyone\\nunderstands  rather than saying are multiple different sources i might say lots of stuff\\nbe willing to negotiate everything except for your values I used to think of negotiating as a zero-sum\\ngame and it's not at all you can find a way where both parties get better than they were before which\\nis the entire point that is how a negotiating works if you're doing it well I didn't understand that I\n",
    "Source2: you through the three values that we have at acquisition.com and why we believe them and  from an\\noverarching perspective the way to come to values  is to look at what are the non-negotiables okay\\nand  what i mean by that is and every company is different because the core thing about values is\\nthat they have to be things that are true and innate to you right  for example at southwest  have fun\\nis one of their core values and if someone does not want to have fun or does not believe in the\\nprocesses that they do to have a good time then that is a non-negotiable for them that person\\ncannot work at that company it is a fireable offense and  these are not aspirational these are not\\nthings that you would  to have these are things that are core to who you are as a person and as a\\nteam all right and  when you decide on these non-negotiables they are by their very nature\\nnon-negotiable which means that you hire based on these you fire based on these and these are the\\ncore spirit of the team and some of the mistakes that i see when people make values is that one\\nthey don't draw the line in the sand and  one of the core things about a value is that it has to you\\nhave to be able to say this is not this right you have to be able to say okay between justice and\\nmercy we lean towards justice or we lean towards mercy which means you have to repel people with\\nyour values all right and  if your values do not repel people then they are not values they are\\nplatitudes okay and  it's very important the next thing is that when you're making your values the\\nvalues themselves need to be ideally said in words that you would normally say  if you have little\\nsayings inside of your community then then a lot of times those can become some of the values you\\nhave  for example jim launch one of our first portfolio companies  speed is king do the boring work\\nthese were different ways i mean we could just say work ethic but that's not the way we would have\\nsaid it we could have say be fast or fast turnaround times but speed is king was the way we would\\nhave said it okay and  some of the things that i've learned with this is also you cannot have too\\nmany values as in i'll say it differently you can't have too many values you have to be very selective\\nat the the true core values of the company all right and  if you're  between two a lot of times you just\\nhave to chunk up  and kind of get a broader value but they become the core and i think that what we\\nhave found is that three is the sweet spot and that number has continued to distill down over time\\nbut i think three is the amount that your human brain can comprehend as lenses to make a decision\n",
    "Task: Based on the source1 text and source2 text, answer the question.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "encoding = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
